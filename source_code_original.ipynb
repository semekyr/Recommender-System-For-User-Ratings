{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load training and testing data\n",
    "def load_data_withoutcsv(training_path, testing_path):\n",
    "    linestraining = [line.rstrip('\\n') for line in open(training_path)]\n",
    "    train_data = []\n",
    "    for line in linestraining: \n",
    "        words = line.split(',')\n",
    "        userid = int(words[0])\n",
    "        itemid = int(words[1])\n",
    "        rating = float(words[2])\n",
    "        train_data.append([userid, itemid, rating])\n",
    "    \n",
    "    test_data = []\n",
    "    linestesting = [line.rstrip('\\n') for line in open(testing_path)]\n",
    "    for line in linestesting:\n",
    "        words = line.split(',')\n",
    "        userid = int(words[0])\n",
    "        itemid = int(words[1])\n",
    "        test_data.append([userid, itemid])\n",
    "\n",
    "    random.shuffle(train_data)\n",
    "    split_index = int(0.8 * len(train_data))\n",
    "    train_data, val_data = train_data[:split_index], train_data[split_index:]\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build the User-Item Rating Matrix\n",
    "def build_user_item_matrix(train_data):\n",
    "    users = sorted(set([d[0] for d in train_data]))\n",
    "    items = sorted(set([d[1] for d in train_data]))\n",
    "\n",
    "    user_map = {u: i for i, u in enumerate(users)}\n",
    "    item_map = {i: j for j, i in enumerate(items)}\n",
    "\n",
    "    matrix = np.zeros((len(users), len(items)))\n",
    "    for user, item, rating in train_data:\n",
    "        matrix[user_map[user], item_map[item]] = rating\n",
    "    \n",
    "    # Compute the User Average Ratings: \n",
    "    user_avg_ratings = np.zeros(len(users))\n",
    "    for user in users:\n",
    "        user_avg_ratings[user_map[user]] = np.mean(matrix[user_map[user], :])\n",
    "\n",
    "    return matrix, user_map, item_map, user_avg_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compute the Cosine Similarity Matrix\n",
    "import numpy as np\n",
    "def adjusted_cosine_similarity(matrix, user_avg_ratings):\n",
    "    # Compute the adjusted cosine similarity matrix\n",
    "    adjusted_matrix = matrix - user_avg_ratings[:, None]\n",
    "    similarity_matrix = adjusted_matrix.dot(adjusted_matrix.T)\n",
    "    norms = np.linalg.norm(adjusted_matrix, axis=1)\n",
    "    similarity_matrix = similarity_matrix / (np.linalg.norm(adjusted_matrix, axis=1)[:, None] @ np.linalg.norm(adjusted_matrix, axis=1)[:, None].T)    \n",
    "    return similarity_matrix\n",
    "\n",
    "def cosine_similarity(matrix):\n",
    "    # Compute the cosine similarity matrix\n",
    "    similarity_matrix = matrix.dot(matrix.T)\n",
    "    similarity_matrix = similarity_matrix / (np.linalg.norm(matrix, axis=1)[:, None] @ np.linalg.norm(matrix, axis=1)[:, None].T)    \n",
    "    return similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Predict Ratings\n",
    "def predict_ratings(userid, itemid, matrix, similarity, user_map, item_map, k=10, similarity_threshold=0.2):\n",
    "    if userid not in user_map or itemid not in item_map:\n",
    "        return 3.0\n",
    "\n",
    "    user_index = user_map[userid]\n",
    "    item_index = item_map[itemid]\n",
    "\n",
    "    if item_index >= similarity.shape[0]:  # Prevent index out-of-bounds error\n",
    "        return 3.0\n",
    "    \n",
    "    # Get similarities for the target item\n",
    "    item_similarity_scores = similarity[item_index]\n",
    "\n",
    "    # Find items the user has rated\n",
    "    rated_items = np.where(matrix[user_index, :] > 0)[0]\n",
    "\n",
    "    if len(rated_items) == 0:\n",
    "        return 3.0  # Default rating\n",
    "    \n",
    "    # Ensure indices are within valid range\n",
    "    valid_rated_items = rated_items[rated_items < similarity.shape[0]]\n",
    "\n",
    "    if len(valid_rated_items) == 0:\n",
    "        return 3.0\n",
    "\n",
    "    # Sort items by similarity and select top-k\n",
    "    sorted_items = valid_rated_items[np.argsort(item_similarity_scores[valid_rated_items])[::-1]]\n",
    "    top_k_items = [item for item in sorted_items if item_similarity_scores[item] > similarity_threshold][:k]\n",
    "\n",
    "    # Compute weighted sum of ratings\n",
    "    numerator = sum([similarity[item_index, item] * matrix[user_index, item] for item in top_k_items])\n",
    "    denominator = sum([similarity[item_index, item] for item in top_k_items])\n",
    "\n",
    "    return round(numerator / denominator) if denominator != 0 else 3.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Generate Predictions for the Test Data\n",
    "def gen_preds(test_dataset, matrix, similarity, user_map, item_map, output):\n",
    "    preds = []\n",
    "    for user, item in test_dataset:\n",
    "        pred = predict_ratings(user, item, matrix, similarity, user_map, item_map)\n",
    "        pred = min(max(pred, 0.5), 5.0)\n",
    "        preds.append([user, item, round(pred, 3)])\n",
    "\n",
    "    with open(output, 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['userid', 'itemid', 'predicted_rating'])\n",
    "        writer.writerows(preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for Evaluation: \n",
    "def MAE(y_pred, y_true):\n",
    "    return np.mean(np.abs(np.array(y_pred) - np.array(y_true)))\n",
    "\n",
    "def RMSE(y_pred, y_true):\n",
    "    return np.sqrt(np.mean((np.array(y_pred) - np.array(y_true))**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets found, data loaded\n",
      "User-item matrix created\n",
      "Similarity computed\n",
      "Similarity adjusted\n",
      "28\n",
      "28\n",
      "k=5, similarity_threshold=0.005, MAE=0.879, RMSE=1.174\n",
      "k=5, similarity_threshold=0.01, MAE=0.879, RMSE=1.174\n",
      "k=5, similarity_threshold=0.015, MAE=0.879, RMSE=1.174\n",
      "k=5, similarity_threshold=0.02, MAE=0.879, RMSE=1.174\n",
      "k=5, similarity_threshold=0.025, MAE=0.879, RMSE=1.174\n",
      "k=5, similarity_threshold=0.03, MAE=0.879, RMSE=1.174\n",
      "k=5, similarity_threshold=0.035, MAE=0.879, RMSE=1.174\n",
      "k=5, similarity_threshold=0.04, MAE=0.879, RMSE=1.174\n",
      "k=5, similarity_threshold=0.045, MAE=0.879, RMSE=1.174\n",
      "k=5, similarity_threshold=0.05, MAE=0.879, RMSE=1.174\n",
      "k=5, similarity_threshold=0.055, MAE=0.879, RMSE=1.174\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user, item, rating \u001b[38;5;129;01min\u001b[39;00m validation_data:\n\u001b[1;32m---> 56\u001b[0m     predicted_rating \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_ratings\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilarity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilarity_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msim_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     val_true\u001b[38;5;241m.\u001b[39mappend(rating)\n\u001b[0;32m     58\u001b[0m     val_pred\u001b[38;5;241m.\u001b[39mappend(predicted_rating)\n",
      "Cell \u001b[1;32mIn[20], line 28\u001b[0m, in \u001b[0;36mpredict_ratings\u001b[1;34m(userid, itemid, matrix, similarity, user_map, item_map, k, similarity_threshold)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m3.0\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Sort items by similarity and select top-k\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m sorted_items \u001b[38;5;241m=\u001b[39m valid_rated_items[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem_similarity_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid_rated_items\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m     29\u001b[0m top_k_items \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sorted_items \u001b[38;5;28;01mif\u001b[39;00m item_similarity_scores[item] \u001b[38;5;241m>\u001b[39m similarity_threshold][:k]\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Compute weighted sum of ratings\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\semel\\anaconda3\\envs\\cuda_pytorch_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1133\u001b[0m, in \u001b[0;36margsort\u001b[1;34m(a, axis, kind, order)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argsort_dispatcher)\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margsort\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;124;03m    Returns the indices that would sort an array.\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \n\u001b[0;32m   1132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margsort\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\semel\\anaconda3\\envs\\cuda_pytorch_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "train_dataset =  'C:/Users/semel/Downloads/socialcomp/train_100k_withratings.csv'\n",
    "test_dataset =  'C:/Users/semel/Downloads/socialcomp/test_100k_withoutratings.csv'\n",
    "# Create the output file:\n",
    "output = 'output.csv'\n",
    "\n",
    "# def case_amp(similarity_matrix, alpha= 2.5):\n",
    "#   return np.sign(similarity_matrix) * np.abs(similarity_matrix) ** alpha\n",
    "\n",
    "\n",
    "train_data, validation_data, test_data = load_data_withoutcsv(train_dataset, test_dataset)\n",
    "print ('Datasets found, data loaded')\n",
    "\n",
    "def variance_weight(similarity_matrix ,matrix):\n",
    "    item_variance = np.var(matrix, axis=0)\n",
    "    weight_matrix = np.sqrt(item_variance[:, None] @ item_variance[None, :])\n",
    "    return similarity_matrix * weight_matrix\n",
    "\n",
    "matrix, user_map, item_map, user_averages = build_user_item_matrix(train_data)\n",
    "print ('User-item matrix created')\n",
    "\n",
    "similarity = adjusted_cosine_similarity(matrix, user_averages)\n",
    "print ('Similarity computed')\n",
    "print ('Similarity adjusted')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val_true = []\n",
    "val_pred = []\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "# Number of neighbors (starts from k=5 and is multiples of 5)\n",
    "k_values = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 150, 200, 250, 300, 350, 400, 450,500]\n",
    "print (len(k_values))\n",
    "# Similarity threshold (starts from k = 0.005 and is multiples of 0.005)\n",
    "similarity_thresholds = [0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045, 0.05, 0.055, 0.06, 0.065, 0.07, 0.075, 0.08, 0.085, 0.09, 0.095, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "print (len(similarity_thresholds))\n",
    "# Store results\n",
    "best_params = None\n",
    "best_mae = float('inf')\n",
    "results = []\n",
    "\n",
    "# Perform grid search\n",
    "for k, sim_threshold in product(k_values, similarity_thresholds):\n",
    "    val_true = []\n",
    "    val_pred = []\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    for user, item, rating in validation_data:\n",
    "        predicted_rating = predict_ratings(user, item, matrix, similarity, user_map, item_map, k=k, similarity_threshold=sim_threshold)\n",
    "        val_true.append(rating)\n",
    "        val_pred.append(predicted_rating)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    mae = MAE(val_pred, val_true)\n",
    "    rmse = RMSE(val_pred, val_true)\n",
    "\n",
    "    print (f\"k={k}, similarity_threshold={sim_threshold}, MAE={mae:.3f}, RMSE={rmse:.3f}\")\n",
    "\n",
    "    # Store result\n",
    "    results.append((k, sim_threshold, mae, rmse))\n",
    "\n",
    "    # Check for best parameters\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        best_params = (k, sim_threshold)\n",
    "\n",
    "# Display results\n",
    "import pandas as pd\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(f\"Best hyperparameters: k={best_params[0]}, similarity_threshold={best_params[1]}\")\n",
    "print(f\"Best MAE: {best_mae:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Once the model is validated, train on the entire dataset and generate predictions\n",
    "# matrix, user_map, item_map = build_user_item_matrix(train_data + validation_data)\n",
    "# similarity = pearson_coefficient(matrix)\n",
    "# gen_preds(test_data, matrix, similarity, user_map, item_map, output)\n",
    "# print (f\"Predictions generated and saved to: {output}\") '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
