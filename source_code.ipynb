{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Step 1: Load the training data'''\n",
    "\n",
    "def load_data(training_path, testing_path):\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "\n",
    "    ''' Load the training dataset:'''\n",
    "    with open(training_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            userid, itemid, rating, _ = row\n",
    "            userid = int(userid)\n",
    "            itemid = int(itemid)\n",
    "            rating = float(rating)\n",
    "            train_data.append([userid, itemid, rating])\n",
    "\n",
    "    ''' Load the testing dataset:'''\n",
    "    with open (testing_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            userid, itemid, _ = row\n",
    "            userid = int(userid)\n",
    "            itemid = int(itemid)\n",
    "            test_data.append([int(userid), int(itemid)])\n",
    "\n",
    "    # Split the data into training and validation sets (80% training, 20% validation), without using external libraries:\n",
    "    random.shuffle(train_data)\n",
    "    split_index = int(0.8 * len(train_data))\n",
    "    train_data, val_data = train_data[:split_index], train_data[split_index:]\n",
    "\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Alternative Step 1 without importing the csv library (CITE stackoverflow solution oops)'''\n",
    "def load_data_withoutcsv(training_path, testing_path):\n",
    "    linestraining = [line.rstrip('\\n') for line in open(training_path)]\n",
    "    train_data = []\n",
    "    for line in linestraining: \n",
    "        words = line.split(',')\n",
    "        userid = int(words[0])\n",
    "        itemid = int(words[1])\n",
    "        rating = float(words[2])\n",
    "        train_data.append([userid, itemid, rating])\n",
    "    \n",
    "    test_data = []\n",
    "    linestesting = [line.rstrip('\\n') for line in open(testing_path)]\n",
    "    for line in linestesting:\n",
    "        words = line.split(',')\n",
    "        userid = int(words[0])\n",
    "        itemid = int(words[1])\n",
    "        test_data.append([userid, itemid])\n",
    "\n",
    "    random.shuffle(train_data)\n",
    "    split_index = int(0.8 * len(train_data))\n",
    "    train_data, val_data = train_data[:split_index], train_data[split_index:]\n",
    "\n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Step 2: Build the User-Item Rating Matrix'''\n",
    "def build_user_item_matrix(train_data):\n",
    "    users = sorted(set([d[0] for d in train_data]))\n",
    "    items = sorted(set([d[1] for d in train_data]))\n",
    "\n",
    "    user_map = {u: i for i, u in enumerate(users)}\n",
    "    item_map = {i: j for j, i in enumerate(items)}\n",
    "\n",
    "    matrix = np.zeros((len(users), len(items)))\n",
    "    for user, item, rating in train_data:\n",
    "        matrix[user_map[user], item_map[item]] = rating\n",
    "\n",
    "    return matrix, user_map, item_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Step 3: Calculate the Similiarity between users'''\n",
    "''' User-based Collaborative Filtering using Pearson Correlation'''\n",
    "def userpearson_similarity(matrix):\n",
    "    num_users, num_items= matrix.shape\n",
    "    similarity = np.zeros((num_users, num_users))\n",
    "    for u1 in range(num_users):\n",
    "        for u2 in range (u1+1, num_users):\n",
    "            # Find common items rated by both users:\n",
    "            common_items = np.where((matrix[u1] > 0) & (matrix[u2] > 0))[0]\n",
    "            # Make sure there are at least 2 common items:\n",
    "            if len(common_items) > 1:\n",
    "                ratings_u1 = matrix[u1, common_items]\n",
    "                ratings_u2 = matrix[u2, common_items]\n",
    "\n",
    "                mean_u1 = np.mean(ratings_u1)\n",
    "                mean_u2 = np.mean(ratings_u2)\n",
    "\n",
    "                # Compute Pearson Correlation:\n",
    "                numerator = np.sum((ratings_u1 - mean_u1) * (ratings_u2 - mean_u2))\n",
    "                denominator = np.sqrt(np.sum((ratings_u1 - mean_u1)**2) * np.sum((ratings_u2 - mean_u2)**2))\n",
    "                similarity[u1, u2] = numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "    return similarity\n",
    "\n",
    "''' Item-based Collaborative filter using Pearson Similarity'''\n",
    "def itempearson_similarity(matrix):\n",
    "    num_items = matrix.shape[1]\n",
    "    similarity = np.zeros((num_items, num_items))\n",
    "\n",
    "    user_means = np.true_divide(matrix.sum(axis = 1), (matrix != 0).sum(axis = 1), where = (matrix != 0).sum(axis = 1) != 0)\n",
    "\n",
    "    for i1 in range(num_items):\n",
    "        for i2 in range (i1 + 1, num_items):\n",
    "            common_users = np.where((matrix[:, i1] > 0) & (matrix[:, i2] > 0))[0]\n",
    "\n",
    "            if len(common_users) > 1: \n",
    "                ratings_i1 = matrix[common_users, i1] - user_means[common_users]\n",
    "                ratings_i2 = matrix[common_users, i2] - user_means[common_users]\n",
    "\n",
    "                numerator = np.sum(ratings_i1 * ratings_i2)\n",
    "                denominator = np.sqrt(np.sum(ratings_i1**2) * np.sum(ratings_i2**2))\n",
    "\n",
    "                similarity[i1, i2] = numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "    return similarity\n",
    "\n",
    "''' User-Based Cosine Similarity'''\n",
    "def cosine_similarity(matrix):\n",
    "    norm = np.linalg.norm(matrix, axis = 1)\n",
    "    norm[norm == 0] = 1\n",
    "    normalized_matrix = matrix / norm[:, None]\n",
    "    similarity = np.dot(normalized_matrix, normalized_matrix.T)\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Step 4: Predict Ratings using Similarity from Step 3'''\n",
    "def predict_ratings(userid, itemid, matrix, similarity, user_map, item_map, k=5, similarity_threshold = 0.1):\n",
    "    # Neighbourhood Selection:\n",
    "    if userid not in user_map or itemid not in item_map:\n",
    "        return 3.0\n",
    "    \n",
    "    user_index = user_map[userid]\n",
    "    item_index = item_map[itemid]\n",
    "\n",
    "    # Compute similarity scores:\n",
    "    user_similarity_scores = similarity[user_index]\n",
    "\n",
    "    # Find users who have rated the item:\n",
    "    rated_item_users = np.where(matrix[:, item_index] > 0)[0]\n",
    "\n",
    "    if len(rated_item_users) == 0:\n",
    "        return 3.0\n",
    "    \n",
    "    # If users found, sort users by similarity and select top-k:\n",
    "    sorted_users = rated_item_users[np.argsort(user_similarity_scores[rated_item_users])[::-1]]\n",
    "    top_k_users = [user for user in sorted_users if user_similarity_scores[user] > similarity_threshold][:k]\n",
    "\n",
    "    # Compute weighted sum of ratings:\n",
    "    numerator = sum([similarity[user_index, user] * matrix[user, item_index] for user in top_k_users])\n",
    "    denominator = sum([similarity[user_index, user] for user in top_k_users])\n",
    "    weighted_sum = numerator/denominator if denominator != 0 else 3.0\n",
    "    return weighted_sum\n",
    "                      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Step 5: Generate the Predictions'''\n",
    "def gen_preds(test_dataset, matrix, similarity, user_map, item_map, output ):\n",
    "    preds = []\n",
    "    for user, item in test_dataset:\n",
    "        pred = predict_ratings(user, item, matrix, similarity, user_map, item_map)\n",
    "        pred = min(max(pred, 0.5), 5.0)\n",
    "        preds.append([user, item, round(pred, 3)])\n",
    "\n",
    "    with open(output, 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['userid', 'itemid', 'predicted_rating'])\n",
    "        writer.writerows(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Helper functions for evaluation'''\n",
    "def MAE(y_pred, y_true):\n",
    "    return np.mean(np.abs(np.array(y_pred) - np.array(y_true)))\n",
    "\n",
    "def RMSE(y_pred, y_true):\n",
    "    return np.sqrt(np.mean((np.array(y_pred) - np.array(y_true))**2))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Helper Functions to Improve Performance'''\n",
    "# 1. Significance Weighting \n",
    "def significance_weight(similarity_matrix, common_items, threshold = 50):\n",
    "    weight_factor = np.minimum(1, common_items) / threshold\n",
    "    return similarity_matrix * weight_factor\n",
    "\n",
    "# 2. Case Amplification to enhance strong neighbours\n",
    "def case_amp(similarity_matrix, alpha = 2.5):\n",
    "    return np.sign(similarity_matrix) * np.abs(similarity_matrix)**alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets found, data loaded\n",
      "User-item matrix created\n",
      "Similarity computed\n",
      "Validation MAE: 0.851\n",
      "Validation RMSE: 1.076\n"
     ]
    }
   ],
   "source": [
    "''' Step 6: Run the Code'''\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "train_dataset =  'C:/Users/semel/Downloads/socialcomp/train_100k_withratings.csv'\n",
    "test_dataset =  'C:/Users/semel/Downloads/socialcomp/test_100k_withoutratings.csv'\n",
    "# Create the output file:\n",
    "output = 'output.csv'\n",
    "\n",
    "train_data, validation_data, test_data = load_data_withoutcsv(train_dataset, test_dataset)\n",
    "print ('Datasets found, data loaded')\n",
    "\n",
    "matrix, user_map, item_map = build_user_item_matrix(train_data)\n",
    "print ('User-item matrix created')\n",
    "\n",
    "similarity = cosine_similarity(matrix)\n",
    "print ('Similarity computed')\n",
    "#similarity = significance_weight(similarity, 50)\n",
    "#similarity = case_amp(similarity, 2.5)\n",
    "\n",
    "val_true = []\n",
    "val_pred = []\n",
    "\n",
    "# Since we don't have the ground truth ratings for the test set, we split the training set into training and validation sets:\n",
    "for user, item , rating in validation_data:\n",
    "    predicted_rating = predict_ratings(user, item, matrix, similarity, user_map, item_map)\n",
    "    val_true.append(rating)\n",
    "    val_pred.append(predicted_rating)\n",
    "\n",
    "validation_MAE = MAE(val_pred, val_true)\n",
    "validation_RMSE = RMSE(val_pred, val_true)\n",
    "print ('Validation MAE:', round(validation_MAE, 3))\n",
    "print ('Validation RMSE:', round(validation_RMSE, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Step 7: Once the model is validated, train on the entire dataset and generate predictions\\nmatrix, user_map, item_map = build_user_item_matrix(train_data + validation_data)\\nsimilarity = pearson_coefficient(matrix)\\ngen_preds(test_data, matrix, similarity, user_map, item_map, output)\\nprint (f\"Predictions generated and saved to: {output}\") '"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Step 7: Once the model is validated, train on the entire dataset and generate predictions\n",
    "matrix, user_map, item_map = build_user_item_matrix(train_data + validation_data)\n",
    "similarity = pearson_coefficient(matrix)\n",
    "gen_preds(test_data, matrix, similarity, user_map, item_map, output)\n",
    "print (f\"Predictions generated and saved to: {output}\") '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
